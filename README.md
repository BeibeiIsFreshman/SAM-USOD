Please criticize and correct me. I will reply to your message as soon as possible.

The USOD10k training set can be downloaded from the publisher of USOD10k. “USOD10K: A New Benchmark Dataset for Underwater Salient Object Detection”

The SAM fine-tuning framework is available on the release site of MDSAM. “Multi-Scale and Detail-Enhanced Segment Anything Model for Salient Object Detection”

The weights will be uploaded in a timely manner once the paper is accepted.

Due to the limited equipment of our model, SAM_B can only be trained at 256 x 256 and SAM_L can only be trained at 224 x 224. If you have good equipment to reproduce this code and get better weights, such as using a higher size or a more powerful SAM original weight.

Abstract— RGB–depth underwater salient object detection (USOD) poses considerable challenges, such as uneven lighting, visual interference, and image blur, which limit the effectiveness of traditional approaches. The segment anything model (SAM), known for its robust segmentation capabilities, offers a promising alternative. However, SAM depends on prompt labels (e.g., points, boxes, masks) to perform effective resources typically unavailable in USOD datasets. To address this, we propose SAM-CLNet, a prompt-free, SAM-enhanced collaborative learning network comprising three main components: (1) SAM, (2) a mask prompt generator (MPG), and (3) a region-aware attention collaborative learning loss (RCL). In our framework, pseudo-mask prompts generated by MPG were used as input prompts for SAM, helping to offset performance degradation due to the absence of manual labels. Simultaneously, RCL leveraged high-quality SAM predictions to refine MPG, enhancing its feature extraction while minimizing the impact of low-quality pseudo-prompts on SAM. This cyclic feedback mechanism facilitated mutual improvement in detection accuracy. In addition, we introduced a U-Adapter module to adapt SAM for underwater imagery and incorporated a frequency cross-attention fusion module in MPG to integrate RGB and depth information. The region-aware attention in RCL further targeted challenging regions by comparing SAM’s predictions with MPG’s pseudo-mask. Experiments on the USOD10K and USOD datasets demonstrated that SAM-CLNet outperformed existing methods and generalized effectively across five public salient object detection benchmarks.

The diagram of our model is as follows:
<img width="876" height="524" alt="image" src="https://github.com/user-attachments/assets/d2796c82-fc08-462f-93ca-eb44fb5cf30f" />

The results of our comparison method are as follows:
<img width="978" height="793" alt="image" src="https://github.com/user-attachments/assets/280a1a7d-5723-4666-8027-815b6187783c" />
<img width="981" height="562" alt="image" src="https://github.com/user-attachments/assets/86da60ee-1dc8-420a-a400-cb5509501f48" />


